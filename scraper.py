import json
import re
from pathlib import Path
from typing import List, Set
from urllib.parse import urlparse, parse_qs, unquote

from playwright.sync_api import sync_playwright, Page

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
SOURCES_FILE = Path("config/sources.json")
SEEN_URLS_FILE = Path("data/seen_urls.json")

# gofile ã® URLãƒ‘ã‚¿ãƒ¼ãƒ³
GOFILE_REGEX = re.compile(r"https://gofile\.io/d/[0-9A-Za-z]+")

# ãƒšãƒ¼ã‚¸å†…ãƒ†ã‚­ã‚¹ãƒˆã«ã“ã®ã©ã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ãƒªãƒ³ã‚¯åˆ‡ã‚Œã¨ã¿ãªã™
GOFILE_DEAD_PATTERNS = [
    "This content does not exist",
    "The content you are looking for could not be found",
    "No items to display",
    "This content is password protected",
]

OREVIDEO_URL = "https://orevideo.pythonanywhere.com/"


def load_sources() -> List[str]:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã® Nitter URL ã‚’ config/sources.json ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    if not SOURCES_FILE.exists():
        raise FileNotFoundError(f"{SOURCES_FILE} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚Nitterã®URLã‚’ã“ã“ã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")
    with SOURCES_FILE.open("r", encoding="utf-8") as f:
        data = json.load(f)
    # å½¢å¼: { "sources": ["https://nitter.net/...", ...] }
    return data.get("sources", [])


def load_seen_urls() -> Set[str]:
    """ã™ã§ã«å‡¦ç†æ¸ˆã¿ (ã¾ãŸã¯æ­»ã‚“ã§ã„ãŸ) gofile URL ã‚’èª­ã¿è¾¼ã¿"""
    if not SEEN_URLS_FILE.exists():
        SEEN_URLS_FILE.parent.mkdir(parents=True, exist_ok=True)
        with SEEN_URLS_FILE.open("w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)
        return set()
    with SEEN_URLS_FILE.open("r", encoding="utf-8") as f:
        try:
            urls = json.load(f)
            return set(urls)
        except json.JSONDecodeError:
            return set()


def save_seen_urls(seen: Set[str]) -> None:
    """å‡¦ç†æ¸ˆã¿ gofile URL ã‚’ JSON ã«ä¿å­˜"""
    SEEN_URLS_FILE.parent.mkdir(parents=True, exist_ok=True)
    with SEEN_URLS_FILE.open("w", encoding="utf-8") as f:
        json.dump(sorted(seen), f, ensure_ascii=False, indent=2)


def extract_gofile_from_href(href: str) -> str | None:
    """
    href ã‹ã‚‰ gofile.io/d/... ã®å®ŸURLã‚’å–ã‚Šå‡ºã™ã€‚
    - ç›´æŽ¥ https://gofile.io/d/XXX ã®å ´åˆ
    - /external?url=https%3A%2F%2Fgofile.io%2Fd%2FXXX ã®ã‚ˆã†ãªå ´åˆ
    """
    if not href:
        return None

    # ã‚±ãƒ¼ã‚¹1: ãã®ã¾ã¾ gofile.io/d/xxx ãŒå…¥ã£ã¦ã„ã‚‹
    m = GOFILE_REGEX.search(href)
    if m:
        return m.group(0)

    # ã‚±ãƒ¼ã‚¹2: ?url= ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    try:
        parsed = urlparse(href)
        qs = parse_qs(parsed.query)
        if "url" in qs:
            for v in qs["url"]:
                decoded = unquote(v)
                m2 = GOFILE_REGEX.search(decoded)
                if m2:
                    return m2.group(0)
    except Exception:
        pass

    return None


def scroll_and_collect_gofile_urls(page: Page) -> Set[str]:
    """
    Nitterãƒšãƒ¼ã‚¸ã§ 'Load more' ã‚’æŠ¼ã—ã¤ã¤ã€gofile.io/d/... URL ã‚’ã™ã¹ã¦åŽé›†ã€‚
    HTMLã‚’æ­£è¦è¡¨ç¾ã§èˆã‚ã‚‹ã®ã§ã¯ãªãã€aã‚¿ã‚°ã®hrefã‚’å…¨éƒ¨è¦‹ã‚‹ã€‚
    """
    urls: Set[str] = set()

    while True:
        link_locators = page.locator("a")
        count = link_locators.count()
        print(f"    Scanning {count} links on this page...")
        for i in range(count):
            try:
                href = link_locators.nth(i).get_attribute("href")
            except Exception:
                continue
            gofile_url = extract_gofile_from_href(href or "")
            if gofile_url:
                urls.add(gofile_url)

        # Load more ãƒœã‚¿ãƒ³ã‚’æŽ¢ã™
        load_more = page.locator("div.show-more a:has-text('Load more')")
        if load_more.count() == 0:
            break

        try:
            print("    Clicking 'Load more'...")
            load_more.first.click()
            # èª­ã¿è¾¼ã¿å¾…ã¡ï¼ˆé©å®œèª¿æ•´ï¼‰
            page.wait_for_timeout(2000)
        except Exception:
            print("    Failed to click 'Load more' or no more pages.")
            break

    return urls


def is_gofile_alive(page: Page, url: str) -> bool:
    """gofile.io ã®ãƒªãƒ³ã‚¯ãŒç”Ÿãã¦ã„ã‚‹ã‹åˆ¤å®š"""
    try:
        page.goto(url, wait_until="networkidle", timeout=30000)
    except Exception:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯ä¸€æ—¦ã€Œç”Ÿãã¦ãªã„ã€ã¨ã¿ãªã™
        return False

    # JSãŒè½ã¡ç€ãã¾ã§å°‘ã—å¾…ã¤
    page.wait_for_timeout(3000)

    try:
        text = page.inner_text("body")
    except Exception:
        return False

    for pattern in GOFILE_DEAD_PATTERNS:
        if pattern in text:
            return False

    return True


def upload_to_orevideo(page: Page, gofile_url: str) -> None:
    """orevideo.pythonanywhere.com ã«å¯¾ã—ã¦URLã‚’é€ä¿¡"""
    page.goto(OREVIDEO_URL, wait_until="domcontentloaded", timeout=30000)
    page.wait_for_timeout(2000)

    # input#url ã«å€¤ã‚’å…¥ã‚Œã‚‹
    page.fill("input#url", gofile_url)

    # ãƒœã‚¿ãƒ³æŠ¼ä¸‹
    page.click("#submitBtn")

    # å‡¦ç†å¾…ã¡ï¼ˆå¿…è¦ã«å¿œã˜ã¦å»¶é•·ï¼‰
    page.wait_for_timeout(5000)


def main():
    sources = load_sources()
    seen_urls = load_seen_urls()

    print(f"Loaded {len(seen_urls)} seen URLs")

    with sync_playwright() as p:
        # headless Chromium ã‚’èµ·å‹•
        browser = p.chromium.launch(headless=True)

        # ðŸ‘‰ User-Agent ã‚’æ™®é€šã®Chromeãƒ–ãƒ©ã‚¦ã‚¶ã£ã½ãå½è£…
        context = browser.new_context(
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            viewport={"width": 1280, "height": 720},
        )

        # Nitterã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ãƒšãƒ¼ã‚¸
        nitter_page = context.new_page()
        # gofileç¢ºèª & orevideoã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒšãƒ¼ã‚¸
        gofile_page = context.new_page()
        ore_page = gofile_page  # åŒã˜ã‚¿ãƒ–ã‚’ä½¿ã„å›žã™

        new_seen = False

        for src in sources:
            print(f"Scraping source: {src}")
            try:
                nitter_page.goto(src, wait_until="networkidle", timeout=30000)
            except Exception as e:
                print(f"  Failed to open {src}: {e}")
                continue

            # æœ€çµ‚URLã¨HTMLã®ä¸€éƒ¨ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            print(f"  Final URL: {nitter_page.url}")
            nitter_page.wait_for_timeout(2000)

            try:
                html = nitter_page.content()
                snippet = html[:600].replace("\n", " ")
                print(f"  Page HTML snippet: {snippet}")
            except Exception as e:
                print(f"  Failed to get page content: {e}")

            urls = scroll_and_collect_gofile_urls(nitter_page)
            print(f"  Found {len(urls)} gofile URLs")

            for url in sorted(urls):  # ä¸€å¿œã‚½ãƒ¼ãƒˆï¼ˆå®‰å®šæ€§ã®ãŸã‚ï¼‰
                if url in seen_urls:
                    # ã™ã§ã«å‡¦ç†æ¸ˆã¿
                    continue

                print(f"  Checking gofile URL: {url}")
                if not is_gofile_alive(gofile_page, url):
                    print("    -> Dead or password protected. Skipped.")
                    # æ­»ã‚“ã§ã„ã‚‹ã‚‚ã®ã‚‚å†ãƒã‚§ãƒƒã‚¯ä¸è¦ãªã‚‰ã“ã“ã§è¿½åŠ 
                    seen_urls.add(url)
                    new_seen = True
                    continue

                print("    -> Alive. Uploading to orevideo...")
                try:
                    upload_to_orevideo(ore_page, url)
                    print("    -> Upload done.")
                    seen_urls.add(url)
                    new_seen = True
                except Exception as e:
                    print(f"    -> Upload failed: {e}")

        browser.close()

    if new_seen:
        save_seen_urls(seen_urls)
        print(f"Saved {len(seen_urls)} seen URLs")
    else:
        print("No new URLs processed.")


if __name__ == "__main__":
    main()
